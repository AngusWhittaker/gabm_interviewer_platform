{% load static %}

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>GABM</title>

    <meta name="description" content="" />

    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="{% static 'gabm/sneat/assets/img/favicon/favicon.ico' %}" />

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Public+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&display=swap"
      rel="stylesheet" />

    <!-- Core CSS -->
    <link rel="stylesheet" href="{% static 'gabm/sneat/assets/vendor/css/rtl/core.css' %}" class="template-customizer-core-css" />
    <link rel="stylesheet" href="{% static 'gabm/sneat/assets/vendor/css/rtl/theme-default.css' %}" class="template-customizer-theme-css" />


    <!-- This is the main css for defining the visualizer -->
    <style>
      body {
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        align-items: center;
        height: 100vh;
        background: linear-gradient(45deg, #6e45e2, #88d3ce);
        font-family: 'Arial', sans-serif;
      }

      button {
        padding: 10px 20px;
        border: none;
        border-radius: 5px;
        background-color: #fff;
        color: #333;
        font-size: 16px;
        cursor: pointer;
        margin: 10px;
        transition: background-color 0.3s, transform 0.2s;
      }

      button:hover {
        background-color: #f8f9fa;
        transform: scale(1.05);
      }

      button:active {
        transform: scale(0.95);
      }

      #visualizer {
        width: 150px;
        height: 150px;
        background-color: #fff;
        border-radius: 50%;
        border: 2px solid #fff;
        position: absolute;
        left: 50%;
        top: 50%;
        transform: translate(-50%, -50%) scale(1);
        transition: transform 0.1s ease;
        box-shadow: 0 0 15px rgba(255, 255, 255, 0.6), 0 0 25px rgba(255, 255, 255, 0.6), 0 0 35px rgba(255, 255, 255, 0.6);

        display: flex;
        justify-content: center;
        align-items: center;
      }

      #visualizer>i {
        position: absolute;
        font-size: 2em;
        color: rgb(110, 129, 255);
      }

      #controls {
        position: fixed;
        bottom: 70px;
        left: 50%;
        transform: translateX(-50%);
      }

      #endButton {
        display: none;
        opacity: 0; /* Start with the button invisible */
        transition: opacity 2s; /* Gradual fade effect over 2 seconds */
        margin-bottom: 0.7em;
      }
    </style>

    <!-- This is CSS specifically for audio play animation when the agent is 
         thinking about its next lines -->
    <style>
      #thinkingIcon {
        display: none;
        justify-content: center;
        align-items: center;
        height: 100vh;

        opacity: 0; /* Make it invisible initially */
        transition: opacity 2s; /* Adjust time as needed */
      }

      .circle {
        width: 24px;
        height: 24px;
        margin: 0 10px;
        background-color: white; /* You can change the color */
        border-radius: 50%;
        animation: wave 1.5s ease-in-out infinite;
        box-shadow: 0 0 15px rgba(255, 255, 255, 0.6), 0 0 25px rgba(255, 255, 255, 0.6), 0 0 35px rgba(255, 255, 255, 0.6);
      }

      .circle:nth-child(2) {
        animation-delay: 0.1s;
      }

      .circle:nth-child(3) {
        animation-delay: 0.2s;
      }

      @keyframes wave {
        0%, 100% {
          transform: translateY(0);
        }
        50% {
          transform: translateY(-20px);
        }
      }
    </style>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" />
  </head>

  <body>
    <div id="controls">
      <button id="startButton" class="btn rounded-pill btn-info"><strong>Click here to start the interview</strong></button>
      <a href="{% url 'home' %}" id="endButton" type="button" class="btn rounded-pill btn-info"><strong>Click here to return to the main page</strong></a>
    </div>
    <div id="visualizer">
      <i id="micIcon" class="fa fa-microphone" style="display: none"></i>
      <img id="speakerIcon" src="{% static 'gabm/img/Isabella_Rodriguez.png' %}" style="display: none" />
      <span id="thinkingIcon">
        <div class="circle"></div>
        <div class="circle"></div>
        <div class="circle"></div>
      </span>
    </div>

    <script>
      // #####################################################################
      // [SECTION 1: SETTING VARIABLES]

      // Handles for HTML elements:
      let visualizerElement = document.getElementById('visualizer');
      let micIcon = document.getElementById('micIcon');
      let speakerIcon = document.getElementById('speakerIcon');

      // Web Audio API objects. 
      // These are the main Audio API objects that will be used throughout the
      // page. These are defined when the user clicks on the "Start" button. 
      let audioContext;
      let agentAnalyser, userAnalyser; 
      let userProcessor;

      // Settings and parameters: 
      const speechThreshold = {{ request.user.audio_calibration_float }};
      // const speechThreshold = 0.02;
      const maxSilenceDuration = 6;
      const fade_exponent = 3;
      const recentRMSsSizes = 10;
      const RPTStartThreshold = 3;
      const RPTEndThreshold = 5; 

      // State variables: 
      let silenceStarted;
      let silenceStartTime;
      let isUserTurn = false;
      let userHasSpoken = false;
      let currRMS;
      let recentRMSs = []
      let currentAudioElement = null;
      let interviewCompleted = false;

      // For recording:
      let mediaRecorder;
      let isLoading = false;

      // Setting up the csrftoken for ajax calls. 
      function getCookie(name) {
        let cookieValue = null;
        if (document.cookie && document.cookie !== '') {
          const cookies = document.cookie.split(';');
          for (let i = 0; i < cookies.length; i++) {
            const cookie = cookies[i].trim();
            // Does this cookie string begin with the name we want?
            if (cookie.substring(0, name.length + 1) === (name + '=')) {
              cookieValue = decodeURIComponent(cookie.substring(name.length + 1));
              break;
            }
          }
        }
        return cookieValue;
      }
      const csrftoken = getCookie('csrftoken');


      // #####################################################################
      // [SECTION 2: DECORATION]

      // Changing the icon visuals to fit the current stage. 
      function updateIcons() {
        if (isLoading) {
          // <Part 1. Icon when the agent is thinking of the next line>
          visualizer.style.backgroundColor = "transparent";
          visualizer.style.borderColor = "transparent";
          visualizer.style.boxShadow = "none";

          micIcon.style.display = 'none';
          speakerIcon.style.display = 'none';
          thinkingIcon.style.display = 'flex';

          // We implement a fade in for the thinking animation. 
          // Grab your container
          var loadingContainer = document.querySelector('#thinkingIcon');
          // Initially hide the container
          loadingContainer.style.display = 'none';
          // Function to fade in
          function fadeInLoading() {
            loadingContainer.style.display = 'flex'; // Set display to flex
            setTimeout(() => {
              loadingContainer.style.opacity = 1; // Start the fade in
            }, 15); // Timeout allows CSS to recognize the display change
          }
          // Trigger the fade in
          fadeInLoading();

        } else if (isUserTurn) {
          // <Part 2. Icon for when the user is speaking>
          visualizer.style.backgroundColor = "white";
          visualizer.style.borderColor = "white";
          visualizer.style.boxShadow = "0 0 15px rgba(255, 255, 255, 0.6), 0 0 25px rgba(255, 255, 255, 0.6), 0 0 35px rgba(255, 255, 255, 0.6)";

          micIcon.style.display = 'block';
          speakerIcon.style.display = 'none';
          thinkingIcon.style.display = 'none';

        } else {
          // <Part 3. Icon for when the agent is speaking>
          visualizer.style.backgroundColor = "white";
          visualizer.style.borderColor = "white";
          visualizer.style.boxShadow = "0 0 15px rgba(255, 255, 255, 0.6), 0 0 25px rgba(255, 255, 255, 0.6), 0 0 35px rgba(255, 255, 255, 0.6)";

          micIcon.style.display = 'none';
          speakerIcon.style.display = 'block';
          thinkingIcon.style.display = 'none';
        }
      }


      // #####################################################################
      // [SECTION 3: CORE FUNCTIONS]

      // Returns the number of recent RMNs that exceeded the speechThreshold. 
      function getRecentPassedThreshold(RMS) {
        // If RMS is higher than the speechThreshold, we add 1 to the 
        // recentRMSs, which is a list of floats (RMSs) with the length of
        // recentRMSsSizes. If it is lower or equal, we add 0. 
        let passedThreshold = 0;
        if (RMS > speechThreshold) {
          console.log(RMS + " :: "  + speechThreshold)
          passedThreshold += 1
        }
        recentRMSs.push(passedThreshold)
        // Remove the oldest entry if there are more than 20 items
        if (recentRMSs.length > recentRMSsSizes) {
          recentRMSs.shift(); 
        }
        // Use the reduce method to sum all numbers
        const recentPassedThreshold = recentRMSs.reduce((accumulator, currentValue) => {
          return accumulator + currentValue;
        }, 0); 
        // This is basically the number of RMSs, among the most recent 
        // recentRMSsSizes RMSs that exceeded the speechThreshold. 
        return recentPassedThreshold
      }


      // Animates the agent and interviewee's voice, and records the 
      // interviewee's voice. 
      function animateAndRecord() {
        window.requestAnimationFrame(animateAndRecord);
        let analyser = isUserTurn ? userAnalyser : agentAnalyser;
        // Create a new Uint8Array to store the frequency data
        let dataArray = new Uint8Array(analyser.frequencyBinCount);
        // Get the frequency data and fill the Uint8Array
        analyser.getByteFrequencyData(dataArray);

        // Note that we calculate two things here: 
        //   1. average of the dataArray.
        //   2. RMS. 
        // 1 is used for visualization, while 2 is used to determine whether
        // the current level of voice reaches the level of intensity that 
        // would make us think that it might contain speech. 

        // Calculate the sum and average of the frequency data
        let sum = dataArray.reduce((a, b) => a + b, 0);
        let average = sum / dataArray.length;

        // Calculate the scale based on the average frequency
        let scale = 1 + (average / 64);
        // Apply the scale transformation to the visualizer element. But this
        // is only if the agent is speaking, or if we have determined that the
        // user started speaking. 
        if (!isUserTurn || userHasSpoken) {
          visualizerElement.style.transform = `translate(-50%, -50%) scale(${scale})`;
        }

        // <User's turn handler>
        // If this is the user's turn, we do a couple of things here: 
        // 1) Make the visualizer circle gradually fade away as the user stops
        //    speaking
        // 2) Start recording if we detect that the user started to talk, and 
        //    end the recording if we determine that they are done. 
        if (isUserTurn) {
          // <Calculating the RMS>
          userProcessor.onaudioprocess = function(audioProcessingEvent) {
            // Get the input buffer from the event
            var inputBuffer = audioProcessingEvent.inputBuffer;
            // Get the raw audio data from the buffer
            var inputData = inputBuffer.getChannelData(0); // for mono channel
            // Calculate RMS
            let sumSquares = 0.0;
            for (let i = 0; i < inputData.length; i++) {
              sumSquares += inputData[i] * inputData[i];
            }
            currRMS = Math.sqrt(sumSquares / inputData.length);
            // console.log("Current RMS: " + currRMS);
          };
          // </Calculating the RMS>

          const recentPassedThreshold = getRecentPassedThreshold(currRMS);

          // <Speech begins>
          // Determining when the speech began, and then start recording.
          if (recentPassedThreshold > RPTStartThreshold && !userHasSpoken) {
            userHasSpoken = true;　// <state change>
            mediaRecorder.start();
          }
          // </Speech begins>

          // <Speech ends>
          // Determining when the speech ended
          if (userHasSpoken) {
            let secSilence;

            // If the silence started for the first time, we set the timer for
            // detecting the length of the silence. 
            if (silenceStarted) {
              let currTime = new Date();
              secSilence = (currTime - silenceStartTime) / 1000; 
            }
            if (currRMS <= speechThreshold) {
              // If the silence seemingly continues... 
              if (silenceStarted == false) {
                silenceStarted = true; 
                silenceStartTime = new Date();
              };
              if (secSilence > maxSilenceDuration) {
                isLoading = true;　// <state change>
                secSilence = 0;　// <state change>
                isUserTurn = false;　// <state change>
                userHasSpoken = false;　// <state change>
                silenceStarted = false; // <state change>
                mediaRecorder.stop();
              };
            } else {
              // If we think the silence ended... 
              if (recentPassedThreshold < RPTEndThreshold) { 
                secSilence = 0;　// <state change>
                silenceStarted = false; // <state change>
              }
            }
            // Implementing the "fade away". It is based on an exponential 
            // function to make it so that it slowly fades at first, but then
            // quickens as it approaches the end. 
            let opacity = Math.max((1 - (secSilence / maxSilenceDuration) ** fade_exponent), 0);
            if (opacity < 0.01) {
              opacity = 0;
            }
            visualizerElement.style.opacity = opacity;
          }
          // </Speech ends>
        }
        // </User's turn handler>

        // Update any icons or graphics you might have
        updateIcons();
      } 


      // Given an audioElement, plays the audio sound. 
      function playAudio(newAudioElement) {
        // Create the audio source from the new audio element
        let agentSource = audioContext.createMediaElementSource(newAudioElement);
        agentSource.connect(agentAnalyser);
        agentAnalyser.connect(audioContext.destination);
        // Check if there's currently playing audio
        if (currentAudioElement && !currentAudioElement.ended) {
            // If there's an audio playing, wait for it to finish before playing the new one
            currentAudioElement.onended = () => {
                newAudioElement.play();
            };
        } else {
            // If there's no audio playing or the current audio has ended, play the new one immediately
            newAudioElement.play();
        }
        // Update the reference to the currently playing audio
        currentAudioElement = newAudioElement;
      }

      // Define a function to show the button
      function showEndButton() {
        var button = document.getElementById("endButton");
        button.style.display = "inline-flex"; // Make the button take up space and potentially visible
        setTimeout(function() { // Timeout to allow for display change to take effect
          button.style.opacity = 1; // Change the opacity to 1 to fade the button in
        }, 10); // Short timeout before starting the opacity transition
      }


      function handleInterviewCompletion() {
        // Function to handle the clean-up process after interview completion
        if (!currentAudioElement || currentAudioElement.ended) {
          // If there is no current audio or the current audio has ended, close the audio context
          audioContext.close();
          showEndButton();
        } else {
          // If there's ongoing audio, wait for it to finish
          currentAudioElement.onended = () => {
              audioContext.close();
              showEndButton();
          };
        }
      }


      // #####################################################################
      // [SECTION 4: MAIN]
      // This is the main recursive loop for the interviewer agent. 
      function take_one_step(data) {
        console.log("Data sent: " + data);

        // Instantiating the POST call to send the data from frontend to the 
        // backend server. 
        fetch('{% url "handler_take_one_step" %}', {
          method: 'POST',
          body: JSON.stringify(data),
          headers:{
            'Content-Type': 'application/json',
            'X-CSRFToken': csrftoken
          }
        })
        .then(response => response.json())
        .then(data => {
          console.log("Data received: " + data);

          if (data["interview_completed"] == true) {
            console.log("Finished");
            interviewCompleted = true; // Set the flag
            handleInterviewCompletion();
          } else {
            // <If interview is not completed>
            // Everything below happens if the current interview is not 
            // completed according to the backend's response. 

            // <Part 1. Playing the interviewer agent's voice audio.>
            // audio_path assumes a path that looks like: "sounds/myAudio.mp3"
            // let audio_path = "{% static 'gabm/interview/audio/' %}" + data["audio_fname"]; 
            let audio_path = data["audio_url"];
            console.log(audio_path); 
            let audioElement = new Audio(audio_path);
            audioElement.crossOrigin = "anonymous";
            playAudio(audioElement);
            isLoading = false;　// <state change>
            animateAndRecord();
            // </Part 1.>

            // <Part 2. Record the user response.>
            if (data["skip_user_utt"] == false) {
              // If skip_user_utt is false, we want to allow users to speak
              // once the audioElement finishes playing. 
              audioElement.onended = function() {
                isUserTurn = true; // <state change>
                userHasSpoken = false;　// <state change>

                let audioChunks = [];
                // Opening up the mediaDevices and making sure that it is
                // supported. 
                if (navigator.mediaDevices.getUserMedia) {
                  navigator.mediaDevices.getUserMedia ({audio: true})
                  .then(function(stream) {
                    const userSource = audioContext.createMediaStreamSource(stream);
                    userProcessor = audioContext.createScriptProcessor(4096, 1, 1);
                    userSource.connect(userAnalyser);
                    userSource.connect(userProcessor);
                    userProcessor.connect(audioContext.destination);

                    // We are initializing MediaRecorder here. Ideally, we 
                    // would receive audio/wav, but it is no longer supported
                    // by the browsers so we will be sending it in webm.
                    mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
                    // Push audio data into chunks array when data is available
                    mediaRecorder.ondataavailable = event => {
                      audioChunks.push(event.data);
                    };
                    // Here, we define the behavior of mediaRecorder once we finish. 
                    mediaRecorder.onstop = () => {
                      // Do something with audio data here, like sending it to 
                      // the server
                      const audioBlob = new Blob(audioChunks, { 'type' : 'audio/webm' });
                      const audioUrl = URL.createObjectURL(audioBlob);
                      const audio = new Audio(audioUrl);

                      // Convert Blob to base64 or binary data and include in newData
                      const reader = new FileReader();
                      reader.readAsDataURL(audioBlob); 
                      reader.onloadend = function() {
                        const base64data = reader.result;
                        // Here you can send the base64data to the server as 
                        // part of newData
                        var newData = {started: false, user_utt: base64data, script_v: "{{script_v}}"};
                        // Recursively call with new or modified data. 
                        // Importantly, this sends the data to the server. 
                        take_one_step(newData); 
                      }
                    };
                  })
                  .catch(function(err) {
                    console.log('The following gUM error occurred: ' + err);
                  }); 
                } else {
                  console.log('getUserMedia not supported on your browser!');
                }
              }
            } else {
              var newData = {started: false, user_utt: null, script_v: "{{script_v}}"};
              take_one_step(newData); 
            }
            // </Part 2.>
          };
          // </If interview is not completed>
        })
        .catch((error) => {
          console.error('Error:', error);
        });
      }


      // #####################################################################
      // [SECTION 5: DEFINING THE START BUTTON]
      // Defining the start button's behavior. 
      document.getElementById('startButton').addEventListener('click', function() {
        // Once the start button is clicked, it disappears.  
        this.style.display = "none";
        // And we set up the Web Audio API variables. 
        if (!audioContext && navigator.mediaDevices.getUserMedia) {
          audioContext = new (window.AudioContext || window.webkitAudioContext)();
          agentAnalyser = audioContext.createAnalyser();
          agentAnalyser.fftSize=2048
          userAnalyser = audioContext.createAnalyser();
          userAnalyser.fftSize=2048
        }

        // data variable is the json that we will be sending to the backend 
        // for this first interaction. 
        let data = {started: true, user_utt: null, script_v: "{{script_v}}"};
        // Start the recursive calls
        take_one_step(data); 
      });


    </script>

  </body>
</html>

















